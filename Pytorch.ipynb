{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.2755e-39, 1.0561e-38, 1.0286e-38],\n",
      "        [8.4490e-39, 1.0102e-38, 9.0919e-39],\n",
      "        [1.0102e-38, 8.9082e-39, 8.4489e-39],\n",
      "        [9.6429e-39, 8.4490e-39, 9.6429e-39],\n",
      "        [9.2755e-39, 1.0286e-38, 9.0919e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7929, 0.4700, 0.3770],\n",
      "        [0.8568, 0.7433, 0.4723],\n",
      "        [0.2556, 0.1994, 0.9560],\n",
      "        [0.4020, 0.0888, 0.2885],\n",
      "        [0.7940, 0.2045, 0.6717]])\n"
     ]
    }
   ],
   "source": [
    "#randomly initialized matrix\n",
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#construct a matrix filled wiht zeros with long datatype\n",
    "x = torch.zeros(5,3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "#construct a tensor directyly from data\n",
    "x = torch.tensor([5.5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.7176, -1.4418,  1.7376],\n",
      "        [-1.1257,  0.5001,  1.4114],\n",
      "        [-0.4501,  0.8548, -0.7822],\n",
      "        [-2.0231,  0.7597,  0.5196],\n",
      "        [-0.3426, -0.0926, -0.4599]])\n"
     ]
    }
   ],
   "source": [
    "#create tensor based on an existing tensor. These methods will reuse properties of the input tensor\n",
    "# e.g dtype, unless new values are provided by user\n",
    "x = x.new_ones(5,3,dtype = torch.double)  #new_* methods take in sizes\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)     #override dtype!\n",
    "print(x)                                       # result has the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2898, -1.4049,  2.5153],\n",
      "        [-1.1159,  0.6482,  2.2011],\n",
      "        [-0.3890,  1.5479, -0.1656],\n",
      "        [-1.6322,  0.9263,  1.2204],\n",
      "        [ 0.1749,  0.0200,  0.3289]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2898, -1.4049,  2.5153],\n",
      "        [-1.1159,  0.6482,  2.2011],\n",
      "        [-0.3890,  1.5479, -0.1656],\n",
      "        [-1.6322,  0.9263,  1.2204],\n",
      "        [ 0.1749,  0.0200,  0.3289]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2898, -1.4049,  2.5153],\n",
       "        [-1.1159,  0.6482,  2.2011],\n",
       "        [-0.3890,  1.5479, -0.1656],\n",
       "        [-1.6322,  0.9263,  1.2204],\n",
       "        [ 0.1749,  0.0200,  0.3289]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x,y, out=result)\n",
    "#print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2898, -1.4049,  2.5153],\n",
      "        [-1.1159,  0.6482,  2.2011],\n",
      "        [-0.3890,  1.5479, -0.1656],\n",
      "        [-1.6322,  0.9263,  1.2204],\n",
      "        [ 0.1749,  0.0200,  0.3289]])\n"
     ]
    }
   ],
   "source": [
    "#add x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Any operation that mutates a tensor in-place is post-fixed with an _. For example: x.copy_(y), x.t_(), will change x.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2886e+00,  3.5096e+00, -1.5340e+00],\n",
       "        [ 6.5013e-04, -8.8368e-01, -1.7240e+00],\n",
       "        [-8.8860e-01, -4.2763e-01, -1.7673e+00],\n",
       "        [ 1.8660e+00, -2.9658e+00, -4.5787e-01],\n",
       "        [ 6.1800e-01,  6.9440e-01, -1.9102e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.5096, -0.8837, -0.4276, -2.9658,  0.6944])\n"
     ]
    }
   ],
   "source": [
    "print(x[:,1])\n",
    "#print(x[1,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.view\n",
    "If there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1). This is a way of telling the library: \"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2990e+00,  2.1880e+00,  1.7839e-01,  8.5589e-01],\n",
      "        [-5.7073e-01,  2.9425e-01,  2.5970e+00, -7.0434e-01],\n",
      "        [ 1.2879e-01, -1.2391e-03,  9.4784e-01, -1.5730e+00],\n",
      "        [-6.4834e-01, -8.1280e-01,  8.1377e-01, -3.9267e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]),\n",
       " tensor([[ 1.2990e+00,  2.1880e+00,  1.7839e-01,  8.5589e-01, -5.7073e-01,\n",
       "           2.9425e-01,  2.5970e+00, -7.0434e-01],\n",
       "         [ 1.2879e-01, -1.2391e-03,  9.4784e-01, -1.5730e+00, -6.4834e-01,\n",
       "          -8.1280e-01,  8.1377e-01, -3.9267e-01]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resizing/Reshaping the tensor\n",
    "x = torch.randn(4,4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "y.size(),z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6926])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.6925826072692871"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if you have only one element tensor, use .item() to the value as python scalars\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting a torch tensor to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting numpy array to torch tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)\n",
    "np.add(a,1,out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOPE\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "else:\n",
    "    print('NOPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aamir\n"
     ]
    }
   ],
   "source": [
    "print('Aamir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "The autograd package provides automatic differentiation for all operation on Tensor. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "#Tensor\n",
    "------\n",
    "torch.Tensor is central class of the package.\n",
    "If you set its attribute **.requires_grad()** as True, it starts to track all operation on it. When you finish your computtion you can call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated inot .grad attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call **detach()** to detach it from the computation history,\n",
    "and to prevent future computation from being tracked.\n",
    "\n",
    "To prevent tracking history(and using memory), you can also wrap the code block with **torch.no_grad()**: This can be particularly helpful when evaluating a model because the model may have trainaible parameters with requires_grad=True, but for which we don't need the gradients.\n",
    "\n",
    "There is one more clas which is very important for aut0grad implementation a Function\n",
    "\n",
    "Tensor and Function are interconnected and build up and acyclic graph, that encodes a complete history of computation. Each tensor has a **.grad_fn** attribute that references a Function that has created the Tensor.\n",
    "\n",
    "If you want to compute the derivatives, you can call **.backward()** on a Tensor. If Tensor is scalar you don't need to specify any argumennts to backward(), however if it has more elements, you don't need to specify a gradient, but you need to speciy a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x0000021C83D5EBC8>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000021C83D664C8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "a = ((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s backprop now. Because **out** contains a single scalar, **out.backward()** is equivalent to **out.backward(torch.tensor(1.))**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print gradients d(out)/dx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  653.4516,   -56.6601, -1231.5581], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm()<1000:\n",
    "    y=y*2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensor with **.require_grad=True** either\n",
    "by wrapping the code block in **with torch.no_grad()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by using **.detach()** to get new Tensor with some content but \n",
    "that does not require gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28908520.949068658\n",
      "1 23467604.360672787\n",
      "2 24180917.356898397\n",
      "3 27002666.35619718\n",
      "4 28550498.96604411\n",
      "5 25857656.98739277\n",
      "6 19237609.258243036\n",
      "7 11752405.832338084\n",
      "8 6338364.31508195\n",
      "9 3297303.9644351555\n",
      "10 1821877.940887975\n",
      "11 1122950.040700867\n",
      "12 778875.7445060022\n",
      "13 592255.93558374\n",
      "14 478107.95307557227\n",
      "15 399650.0609622641\n",
      "16 340853.999475639\n",
      "17 294273.50134341273\n",
      "18 256055.60325623833\n",
      "19 224053.8768121822\n",
      "20 196932.41476286325\n",
      "21 173753.88637135288\n",
      "22 153835.66916082596\n",
      "23 136607.1900396084\n",
      "24 121657.15584547547\n",
      "25 108619.8864984327\n",
      "26 97210.87079362725\n",
      "27 87206.31840261898\n",
      "28 78402.41101166014\n",
      "29 70627.45804304184\n",
      "30 63747.25632857827\n",
      "31 57638.514810671135\n",
      "32 52204.18377443318\n",
      "33 47363.419667075956\n",
      "34 43037.30964485081\n",
      "35 39165.51339254706\n",
      "36 35694.79114108816\n",
      "37 32572.775689395705\n",
      "38 29762.548943030066\n",
      "39 27229.1400082502\n",
      "40 24942.82337302801\n",
      "41 22872.852175284796\n",
      "42 20996.880012215217\n",
      "43 19294.605288538925\n",
      "44 17748.64421685002\n",
      "45 16341.976654786355\n",
      "46 15061.15350928661\n",
      "47 13893.247725724772\n",
      "48 12826.594160853156\n",
      "49 11851.616057390267\n",
      "50 10959.262081219733\n",
      "51 10142.002439998596\n",
      "52 9392.692091263665\n",
      "53 8705.51088138224\n",
      "54 8075.104312108693\n",
      "55 7495.333325608382\n",
      "56 6961.672904185413\n",
      "57 6470.13658851693\n",
      "58 6016.976081026893\n",
      "59 5599.102284734007\n",
      "60 5213.19659776671\n",
      "61 4856.665916058274\n",
      "62 4527.086292214735\n",
      "63 4221.964458993052\n",
      "64 3939.5939476420454\n",
      "65 3677.8369044139195\n",
      "66 3435.1750690414005\n",
      "67 3210.0670774956993\n",
      "68 3001.049722870969\n",
      "69 2806.952066746153\n",
      "70 2626.646113163196\n",
      "71 2458.8711594671777\n",
      "72 2302.7603856997594\n",
      "73 2157.7134938379477\n",
      "74 2022.6478816528252\n",
      "75 1896.7152790397718\n",
      "76 1779.3215559290427\n",
      "77 1669.7714945080668\n",
      "78 1567.5339446925173\n",
      "79 1472.0523835894755\n",
      "80 1382.8235492188642\n",
      "81 1299.4364520987733\n",
      "82 1221.482452666166\n",
      "83 1148.5730074803062\n",
      "84 1080.3542801215767\n",
      "85 1016.4870541879263\n",
      "86 956.657524381562\n",
      "87 900.6053620403316\n",
      "88 848.0737059176117\n",
      "89 798.8322138221818\n",
      "90 752.6466380443346\n",
      "91 709.3159084833574\n",
      "92 668.6499023666142\n",
      "93 630.4701253096471\n",
      "94 594.6144530717745\n",
      "95 560.9299716597942\n",
      "96 529.2880098788931\n",
      "97 499.53870848941204\n",
      "98 471.58097546665624\n",
      "99 445.2898929943457\n",
      "100 420.5580182106756\n",
      "101 397.2939216352419\n",
      "102 375.37752969792064\n",
      "103 354.7463922321846\n",
      "104 335.3144434597349\n",
      "105 317.0040913576969\n",
      "106 299.75085877735853\n",
      "107 283.48821698907136\n",
      "108 268.15567310020595\n",
      "109 253.69817151556873\n",
      "110 240.06408084704253\n",
      "111 227.1977152414869\n",
      "112 215.05859487706087\n",
      "113 203.597493501297\n",
      "114 192.7791293674863\n",
      "115 182.5620951195933\n",
      "116 172.91209613309184\n",
      "117 163.79658987483305\n",
      "118 155.18464404885427\n",
      "119 147.04722284227174\n",
      "120 139.3566115504774\n",
      "121 132.0849706049398\n",
      "122 125.2086352256407\n",
      "123 118.70602362770089\n",
      "124 112.55694236893804\n",
      "125 106.738372692211\n",
      "126 101.23292183107867\n",
      "127 96.02312225662607\n",
      "128 91.09176377846305\n",
      "129 86.42534697622935\n",
      "130 82.00787141102276\n",
      "131 77.82338574086597\n",
      "132 73.8603903202714\n",
      "133 70.10704509017907\n",
      "134 66.55265619002932\n",
      "135 63.18336200003006\n",
      "136 59.991201546340164\n",
      "137 56.96563480222545\n",
      "138 54.09828274424623\n",
      "139 51.37954394281996\n",
      "140 48.803061396707896\n",
      "141 46.3593914632784\n",
      "142 44.0429731990405\n",
      "143 41.84493958757584\n",
      "144 39.7603576405198\n",
      "145 37.78265106529656\n",
      "146 35.90656835317282\n",
      "147 34.126281754273336\n",
      "148 32.43714457194645\n",
      "149 30.833879549515178\n",
      "150 29.31271488265875\n",
      "151 27.868278482697157\n",
      "152 26.496916654857614\n",
      "153 25.194910717903138\n",
      "154 23.958945888012174\n",
      "155 22.78518764383879\n",
      "156 21.670247288434698\n",
      "157 20.61126184346857\n",
      "158 19.605521463960855\n",
      "159 18.65012648152443\n",
      "160 17.742492060882068\n",
      "161 16.880092300312693\n",
      "162 16.060587589366083\n",
      "163 15.28177068233802\n",
      "164 14.541666332961956\n",
      "165 13.838254558317804\n",
      "166 13.169724415462415\n",
      "167 12.534288333909856\n",
      "168 11.930170878101702\n",
      "169 11.35587486646807\n",
      "170 10.809760170851277\n",
      "171 10.290419502873151\n",
      "172 9.79657305669248\n",
      "173 9.3269415865697\n",
      "174 8.880353612003123\n",
      "175 8.455541091805415\n",
      "176 8.051452931385287\n",
      "177 7.6671172828450835\n",
      "178 7.301565856554867\n",
      "179 6.95367317313573\n",
      "180 6.62270009320347\n",
      "181 6.307815691053273\n",
      "182 6.008134208980391\n",
      "183 5.722969488257511\n",
      "184 5.451605703149475\n",
      "185 5.193353129632684\n",
      "186 4.947544996974511\n",
      "187 4.713662117737941\n",
      "188 4.490984854827317\n",
      "189 4.278975341803196\n",
      "190 4.077161556830129\n",
      "191 3.885045343663249\n",
      "192 3.702117919344488\n",
      "193 3.5279459633071557\n",
      "194 3.3621224763930955\n",
      "195 3.204237492073072\n",
      "196 3.0539107538796135\n",
      "197 2.910708853048439\n",
      "198 2.7743265677736164\n",
      "199 2.644450525620359\n",
      "200 2.5207529129979283\n",
      "201 2.4029284429998334\n",
      "202 2.2906946822024215\n",
      "203 2.1837784152898667\n",
      "204 2.0819270326753347\n",
      "205 1.9849344027735591\n",
      "206 1.8924973023295715\n",
      "207 1.804424434869643\n",
      "208 1.720517886751998\n",
      "209 1.6405703120543338\n",
      "210 1.564383112236948\n",
      "211 1.4917851192014346\n",
      "212 1.422611637927143\n",
      "213 1.3566860887611538\n",
      "214 1.2938773800056125\n",
      "215 1.2340018885147468\n",
      "216 1.1769394230106078\n",
      "217 1.122544457995613\n",
      "218 1.0706956707095217\n",
      "219 1.0212786869782868\n",
      "220 0.9741679008590092\n",
      "221 0.9292619129301039\n",
      "222 0.886453665622798\n",
      "223 0.8456613169052838\n",
      "224 0.8067499566596092\n",
      "225 0.7696552460479251\n",
      "226 0.7342893178480911\n",
      "227 0.7005663739467813\n",
      "228 0.6684119240402749\n",
      "229 0.6377532426106345\n",
      "230 0.6085187387836037\n",
      "231 0.5806415995085479\n",
      "232 0.5540640449660104\n",
      "233 0.5287083986288776\n",
      "234 0.5045308697101294\n",
      "235 0.4814701468733179\n",
      "236 0.45947700189455115\n",
      "237 0.4384981517001221\n",
      "238 0.4184911576920437\n",
      "239 0.3994065234340899\n",
      "240 0.3812029912277326\n",
      "241 0.36384152072905085\n",
      "242 0.3472762976228271\n",
      "243 0.33147371889464583\n",
      "244 0.3163978160314142\n",
      "245 0.3020168168384971\n",
      "246 0.2882975284773386\n",
      "247 0.27520666215155737\n",
      "248 0.2627168458419916\n",
      "249 0.2508023937800842\n",
      "250 0.23943414888946313\n",
      "251 0.22858488570210875\n",
      "252 0.21823287137907038\n",
      "253 0.20835501282651833\n",
      "254 0.19892908067833245\n",
      "255 0.18993398257168903\n",
      "256 0.181349861116476\n",
      "257 0.17315751054625755\n",
      "258 0.16534100763456797\n",
      "259 0.1578803238425997\n",
      "260 0.15075903426570014\n",
      "261 0.14396221398050915\n",
      "262 0.13747561398958505\n",
      "263 0.13128343264253284\n",
      "264 0.12537310410733465\n",
      "265 0.1197319020498919\n",
      "266 0.11434693620161093\n",
      "267 0.10920796951524026\n",
      "268 0.10430132967729172\n",
      "269 0.09961764666095115\n",
      "270 0.09514578058481493\n",
      "271 0.09087662587435427\n",
      "272 0.08680085495239345\n",
      "273 0.08290954797843836\n",
      "274 0.07919446044575205\n",
      "275 0.07564802110173606\n",
      "276 0.07226212104549712\n",
      "277 0.06902873152500039\n",
      "278 0.06594149383784813\n",
      "279 0.06299334352147336\n",
      "280 0.060178213287620956\n",
      "281 0.057490253888240736\n",
      "282 0.054923306787715155\n",
      "283 0.05247210514804325\n",
      "284 0.050131823890199614\n",
      "285 0.04789671851901324\n",
      "286 0.04576208442945994\n",
      "287 0.043723361432799\n",
      "288 0.04177622398994016\n",
      "289 0.039916531765439245\n",
      "290 0.03814041231493536\n",
      "291 0.036444062592718415\n",
      "292 0.034823807727497554\n",
      "293 0.03327668584688743\n",
      "294 0.031798623623603244\n",
      "295 0.03038672018335882\n",
      "296 0.029038049808609412\n",
      "297 0.027749754912509247\n",
      "298 0.026519065241614095\n",
      "299 0.025343417275142537\n",
      "300 0.02422040147998924\n",
      "301 0.023147699273686893\n",
      "302 0.022122915872229636\n",
      "303 0.02114383430868621\n",
      "304 0.02020851887389181\n",
      "305 0.019314815805699673\n",
      "306 0.01846097528115956\n",
      "307 0.01764518446851795\n",
      "308 0.01686571665119601\n",
      "309 0.016120981719894747\n",
      "310 0.01540961439863363\n",
      "311 0.014729699876836994\n",
      "312 0.014080070643223525\n",
      "313 0.013459357363160744\n",
      "314 0.012866152067098097\n",
      "315 0.01229931726475592\n",
      "316 0.011757675132426504\n",
      "317 0.011240073403433165\n",
      "318 0.010745496373301102\n",
      "319 0.010272868953996778\n",
      "320 0.009821108717198004\n",
      "321 0.009389403975557343\n",
      "322 0.008976838699949304\n",
      "323 0.00858251893646686\n",
      "324 0.00820563941443666\n",
      "325 0.007845454629591068\n",
      "326 0.007501221435772075\n",
      "327 0.007172284761774925\n",
      "328 0.006857788766959135\n",
      "329 0.006557198664401364\n",
      "330 0.00626988390591486\n",
      "331 0.005995260630537679\n",
      "332 0.005732755103237248\n",
      "333 0.00548182062565894\n",
      "334 0.005241950237126887\n",
      "335 0.005012693242767298\n",
      "336 0.004793531796964097\n",
      "337 0.004583995527068974\n",
      "338 0.004383693795239987\n",
      "339 0.004192217008363599\n",
      "340 0.004009166042058374\n",
      "341 0.0038341663754928555\n",
      "342 0.0036668472850351404\n",
      "343 0.0035068842339721984\n",
      "344 0.0033539863528233896\n",
      "345 0.0032077694738805677\n",
      "346 0.003067966268234811\n",
      "347 0.0029343066115293036\n",
      "348 0.002806511150710282\n",
      "349 0.002684318098992755\n",
      "350 0.0025674818716101785\n",
      "351 0.002455777119689314\n",
      "352 0.002348970516318378\n",
      "353 0.0022468378361554424\n",
      "354 0.002149166631429014\n",
      "355 0.0020557704468188815\n",
      "356 0.0019664636471937984\n",
      "357 0.0018810621036426088\n",
      "358 0.0017993923335546322\n",
      "359 0.001721292257522129\n",
      "360 0.0016466091286687296\n",
      "361 0.001575206176070174\n",
      "362 0.00150689830567215\n",
      "363 0.001441573393922045\n",
      "364 0.0013791006190066036\n",
      "365 0.0013193527467343386\n",
      "366 0.001262207311327751\n",
      "367 0.0012075560504637452\n",
      "368 0.0011552840675812814\n",
      "369 0.0011053018997140558\n",
      "370 0.0010574861073467398\n",
      "371 0.0010117530996195314\n",
      "372 0.000968009381457054\n",
      "373 0.000926171144485318\n",
      "374 0.000886149888775008\n",
      "375 0.0008478688549804659\n",
      "376 0.0008112553893824722\n",
      "377 0.000776235543082208\n",
      "378 0.0007427331314645393\n",
      "379 0.0007106842491471842\n",
      "380 0.0006800276943180727\n",
      "381 0.0006507011165140615\n",
      "382 0.0006226478056345235\n",
      "383 0.0005958097311487514\n",
      "384 0.000570135003934202\n",
      "385 0.0005455755706811488\n",
      "386 0.0005220822398271308\n",
      "387 0.0004996028831316571\n",
      "388 0.0004780971637915485\n",
      "389 0.0004575229292886377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 0.00043783973665931545\n",
      "391 0.00041900722799718254\n",
      "392 0.00040098974735705876\n",
      "393 0.0003837534015179692\n",
      "394 0.0003672635550018819\n",
      "395 0.0003514830412470171\n",
      "396 0.0003363848693194841\n",
      "397 0.00032193895590575735\n",
      "398 0.0003081169395572833\n",
      "399 0.00029489161575986045\n",
      "400 0.0002822372642342181\n",
      "401 0.00027012862822442453\n",
      "402 0.00025854464950753167\n",
      "403 0.0002474583728037605\n",
      "404 0.00023685003471529453\n",
      "405 0.0002266990829990971\n",
      "406 0.00021698552300931245\n",
      "407 0.0002076900656534485\n",
      "408 0.00019879483482071605\n",
      "409 0.00019028275795681613\n",
      "410 0.00018213856623982495\n",
      "411 0.00017434348005724716\n",
      "412 0.00016688396507548675\n",
      "413 0.00015974567033727704\n",
      "414 0.0001529138939548298\n",
      "415 0.00014637561630650276\n",
      "416 0.00014011870794858477\n",
      "417 0.00013413013894725257\n",
      "418 0.0001283997824518895\n",
      "419 0.00012291496195410414\n",
      "420 0.0001176652815190489\n",
      "421 0.0001126411408454205\n",
      "422 0.00010783253720918066\n",
      "423 0.00010323018417235174\n",
      "424 9.882513644238665e-05\n",
      "425 9.460902871447625e-05\n",
      "426 9.057431480129766e-05\n",
      "427 8.671210601349919e-05\n",
      "428 8.301541314687407e-05\n",
      "429 7.947703307801035e-05\n",
      "430 7.609011100375904e-05\n",
      "431 7.284812278395107e-05\n",
      "432 6.974507429389106e-05\n",
      "433 6.677483620420967e-05\n",
      "434 6.393192625716424e-05\n",
      "435 6.121046572593958e-05\n",
      "436 5.8605206795720886e-05\n",
      "437 5.611148351872356e-05\n",
      "438 5.372419564298634e-05\n",
      "439 5.1439001084946186e-05\n",
      "440 4.925148576156843e-05\n",
      "441 4.715739570880304e-05\n",
      "442 4.5153007404159764e-05\n",
      "443 4.3234028692205356e-05\n",
      "444 4.139688728540755e-05\n",
      "445 3.9638192403075896e-05\n",
      "446 3.7954495779549856e-05\n",
      "447 3.634262412710941e-05\n",
      "448 3.47995113194415e-05\n",
      "449 3.332234320593789e-05\n",
      "450 3.190823427934497e-05\n",
      "451 3.055424662925959e-05\n",
      "452 2.9257953679059668e-05\n",
      "453 2.8016904242833914e-05\n",
      "454 2.6828671106925757e-05\n",
      "455 2.5691020951747497e-05\n",
      "456 2.460183812595551e-05\n",
      "457 2.3559007494082628e-05\n",
      "458 2.2560702924278274e-05\n",
      "459 2.160480673875771e-05\n",
      "460 2.068954840186916e-05\n",
      "461 1.9813190438290803e-05\n",
      "462 1.897410903206288e-05\n",
      "463 1.817070526554253e-05\n",
      "464 1.740145489336869e-05\n",
      "465 1.6664896694870677e-05\n",
      "466 1.5959784830603234e-05\n",
      "467 1.528454904151654e-05\n",
      "468 1.4637934424453869e-05\n",
      "469 1.4018783701740748e-05\n",
      "470 1.3425927100871809e-05\n",
      "471 1.2858229430997947e-05\n",
      "472 1.2314634787574848e-05\n",
      "473 1.1794121677697555e-05\n",
      "474 1.1295769415261466e-05\n",
      "475 1.081847876796343e-05\n",
      "476 1.0361420869790019e-05\n",
      "477 9.923747530083794e-06\n",
      "478 9.504627409429493e-06\n",
      "479 9.103272629568729e-06\n",
      "480 8.718954144614768e-06\n",
      "481 8.350913180653361e-06\n",
      "482 7.998505982628136e-06\n",
      "483 7.660973312126497e-06\n",
      "484 7.337732295186059e-06\n",
      "485 7.028193895443314e-06\n",
      "486 6.731760261256793e-06\n",
      "487 6.447867394274651e-06\n",
      "488 6.175997762620067e-06\n",
      "489 5.9156398382066875e-06\n",
      "490 5.666300950953333e-06\n",
      "491 5.427480944585086e-06\n",
      "492 5.198760958435038e-06\n",
      "493 4.979710183287208e-06\n",
      "494 4.7699316194713165e-06\n",
      "495 4.56901313322501e-06\n",
      "496 4.376592104188457e-06\n",
      "497 4.192307178363872e-06\n",
      "498 4.015810986233817e-06\n",
      "499 3.846753406639762e-06\n"
     ]
    }
   ],
   "source": [
    "'''N is batch size; D_in is input dimension\n",
    "H is hidden dimension; D_out is output dimension'''\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h,0)\n",
    "    #print(h_relu)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t,loss)\n",
    "    \n",
    "    #Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.float32\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "print(t.device)\n",
    "print(t.dtype)\n",
    "print(t.layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor** is class contructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    .tensor is factory function builds tensor object. functions accepts as parameter input and returns a particular type of object. Factory function is an OOP concept which help in creating object instead from the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns Idnetity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor(data) #constructor\n",
    "t2 = torch.tensor(data) #factory func\n",
    "t3 = torch.as_tensor(data)#factory functions\n",
    "t4 = torch.from_numpy(data)#factory func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    " print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.int32\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    " print(t1.dtype)\n",
    "print(t2.dtype)\n",
    "print(t3.dtype)\n",
    "print(t4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dtype() infer based on the incoming data\n",
    "\n",
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor(data)\n",
    "t2 = torch.Tensor(data)\n",
    "t3 = torch.as_tensor(data)#takes any arraya\n",
    "t4 = torch.from_numpy(data)#in this case it only takes from numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]=0\n",
    "data[1]=0\n",
    "data[2]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **t1&t2** it cretes an additional copy where as in\n",
    "case of **t3&t4** it shares a data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations\n",
    "1. Reshaping operations\n",
    "2. Element wise operation\n",
    "3. Reduction operation\n",
    "4. Access operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(t.shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(1,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 2., 2.],\n",
       "        [2., 2., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 2., 2.],\n",
       "        [2., 2., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squeeze & unsqueeze\n",
    "Next way to change the shape is by **squezzing** and **unsquezing** them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeezing** a tensor removes all the axis that have a length of one while **unsqueezing** a tensor adds dimension with a length of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since input tensor can be of any shape we pass the **-1** of reshape function -1 tells to figure out what the value should be based on the other value and the number of the value contained within the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = t.reshape(1,-1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After squeezing the first axis is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Flatten operation Visualized - Tensor batch procession for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Pic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack((t1,t2,t3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = t.reshape(3,1,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1, 1],\n",
       "          [1, 1, 1, 1],\n",
       "          [1, 1, 1, 1],\n",
       "          [1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[2, 2, 2, 2],\n",
       "          [2, 2, 2, 2],\n",
       "          [2, 2, 2, 2],\n",
       "          [2, 2, 2, 2]]],\n",
       "\n",
       "\n",
       "        [[[3, 3, 3, 3],\n",
       "          [3, 3, 3, 3],\n",
       "          [3, 3, 3, 3],\n",
       "          [3, 3, 3, 3]]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(t.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten(start_dim =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_todcast_to(2,t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + torch.tensor(\n",
    "    np.broadcast_to(2,t.shape), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [2,3]\n",
    "\n",
    "    ],dtype=torch.float32\n",
    ")\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even though they have different dimension the elment wise opearion\n",
    "is possible. And braodcasting is what makes it possible.\n",
    "Lower rank **t2** transform into the higher rank tensor **t1** via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 4.],\n",
       "       [2., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(t2.numpy(), t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 6.],\n",
       "        [4., 7.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,6,8],\n",
    "    [8,9,7],\n",
    "    [1,5,3]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ge(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [False,  True,  True]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False, False, False],\n",
       "        [ True, False, False]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False, False, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.le(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False],\n",
       "         [False, False, False],\n",
       "         [ True, False,  True]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It can also be written as\n",
    "\n",
    "t <= torch.tensor([\n",
    "    np.broadcast_to(3,t.shape)\n",
    "],dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 6., 8.],\n",
       "        [8., 9., 7.],\n",
       "        [1., 5., 3.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.4495, 2.8284],\n",
       "        [2.8284, 3.0000, 2.6458],\n",
       "        [1.0000, 2.2361, 1.7321]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -6., -8.],\n",
       "        [-8., -9., -7.],\n",
       "        [-1., -5., -3.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 6., 8.],\n",
       "        [8., 9., 7.],\n",
       "        [1., 5., 3.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor reduction operation\n",
    "A reduction operation on a tensor is an operation \n",
    "that reduces the number of elements contained within\n",
    "the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [5,5,7],\n",
    "    [5,6,9],\n",
    "    [2,5,3]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47.)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum().numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all are reduction operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1417500.)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2222)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1417500.)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0480)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7120,  0.3667, -1.0627,  0.6710],\n",
       "        [-0.9820, -1.2950,  1.8561, -0.9613],\n",
       "        [-0.2291, -0.3647, -0.6109, -1.3317]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4991, -1.2930,  0.1825, -1.6220])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1602, 0.1732, 1.2050, 0.8590])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.prod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1602, 0.1732, 1.2050, 0.8590])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.prod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6871)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.3821)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.5364)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6871, -1.3821, -2.5364])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8561)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns the position of a flatten tensor\n",
    "t.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7120,  0.3667, -1.0627,  0.6710, -0.9820, -1.2950,  1.8561, -0.9613,\n",
       "        -0.2291, -0.3647, -0.6109, -1.3317])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7120, 0.3667, 1.8561, 0.6710]),\n",
       "indices=tensor([0, 0, 1, 0]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 0.7120,  1.8561, -0.2291]),\n",
       "indices=tensor([0, 2, 0]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 0])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8782833333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct/len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No of labels(class name) in the tensor\n",
    "train_set.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frequencey distribution of each class.\n",
    "train_set.train_labels.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''iter creates an iterable object with next \n",
    "function python gets the next data element.'''\n",
    "sample = next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.squeeze()\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(label).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-da52bbeda855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(),cmap='gray')\n",
    "print('label',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have 10 images 1 color channel with 28*28 size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class label 1 for each of 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the whole batch of images at once using the **torchvision**\n",
    "**make_grid** utility function <br>\n",
    "**torchvision.utils.make_grid()**<br>\n",
    "The reason for using plt.imshow(np.transpose(grid, (1,2,0))):\n",
    "For a colored image... plt.imshow takes image dimension in following form [height width channels] ...while pytorch follows [channels height width]... so for compatibility we have to change pytorch dimensions so that channels appear at end... the standard representation of array is [axis0 axis1 axis2].... so we have to convert (0,1,2) to (1,2,0) form to make it compatible for imshow...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7f31157a0b90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#no of images in a row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "grid = torchvision.utils.make_grid(images, nrow=10) #no of images in a row\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.transpose(grid,(1,2,0)))\n",
    "print('labels',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layer = None\n",
    "    def forward(self,t):\n",
    "        t = self.layer(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detail knowledge about this code \n",
    "click this link https://deeplizard.com/learn/video/MasG7tZj-hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first axis is number of filter, whereas 2nd axis is<br>\n",
    "depth or channels while remaining two axis is height and width of the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 3.4887e-02, -3.0878e-02,  5.1553e-02,  2.7052e-03,  1.4350e-01],\n",
       "          [ 1.3531e-01, -1.4501e-01,  5.0909e-02, -4.1123e-02,  1.9747e-01],\n",
       "          [ 1.7528e-03,  1.5012e-01,  1.1205e-01,  1.5753e-01,  8.1572e-02],\n",
       "          [ 1.2380e-01,  5.5811e-02,  1.1895e-01,  2.6236e-02,  6.6441e-03],\n",
       "          [ 1.0526e-01,  5.2157e-02, -6.6079e-02, -4.6008e-02, -1.7273e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4764e-01, -1.4205e-01, -1.6880e-01, -1.0852e-01,  7.7081e-02],\n",
       "          [ 3.7139e-02, -1.4303e-01,  3.1370e-03,  1.7008e-01,  7.4001e-02],\n",
       "          [-1.9204e-01,  8.9254e-02,  1.8077e-01, -1.4127e-01, -1.8067e-01],\n",
       "          [-1.8165e-03, -9.9272e-02, -1.7565e-01,  7.3230e-02,  2.5156e-02],\n",
       "          [-1.2152e-01,  1.5556e-01, -6.3379e-02, -6.6295e-02, -1.4986e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.3127e-02,  2.2531e-03, -1.7851e-01,  4.9535e-02, -1.0665e-01],\n",
       "          [ 8.2430e-02, -1.0046e-01, -1.2062e-01, -1.0744e-01, -6.0848e-03],\n",
       "          [-4.3746e-02, -7.5312e-02,  9.2755e-02,  3.0502e-02,  1.7917e-01],\n",
       "          [ 6.4299e-05,  4.9660e-02, -6.8870e-03,  1.6600e-01,  1.5957e-01],\n",
       "          [ 5.5384e-02,  8.8544e-02, -5.6954e-02, -7.7776e-02, -1.6857e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3169e-01,  2.6692e-02,  5.5077e-02,  5.8675e-02,  1.7626e-01],\n",
       "          [-1.5105e-01, -1.9027e-01, -1.5731e-01,  1.0327e-01,  3.7103e-02],\n",
       "          [ 4.6100e-02, -9.5829e-02, -1.7604e-01, -1.8783e-01, -6.3116e-02],\n",
       "          [ 1.8958e-01,  5.2149e-02,  1.5746e-01,  6.6577e-02,  7.1540e-03],\n",
       "          [-8.6814e-02,  4.6867e-02,  4.1140e-02, -2.2681e-02, -1.3969e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5626e-02, -3.9944e-02, -1.6268e-01,  1.6723e-01,  8.1677e-02],\n",
       "          [-1.0948e-01, -3.0301e-02, -2.4827e-02,  6.7062e-02, -1.4477e-01],\n",
       "          [-1.1264e-01,  7.2726e-02,  3.8837e-02,  6.6030e-03,  9.5677e-02],\n",
       "          [ 1.1564e-02,  1.4406e-01, -1.1707e-01, -1.4530e-01,  1.8752e-01],\n",
       "          [-3.6578e-02,  1.5377e-01,  1.6026e-01, -1.5096e-01,  1.7801e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9397e-01,  1.5147e-01, -8.5302e-02,  5.1324e-02,  1.3922e-03],\n",
       "          [-4.1783e-02, -1.6545e-01,  1.6868e-01,  7.9210e-02,  1.2138e-01],\n",
       "          [-1.7776e-01, -1.0337e-01,  1.6397e-01, -6.1321e-04, -1.6414e-01],\n",
       "          [ 1.5185e-01,  1.1324e-01,  1.7760e-01, -1.7474e-01,  6.4709e-02],\n",
       "          [-6.2079e-02,  1.6905e-01, -1.0209e-01, -1.0455e-01, -6.2142e-02]]]], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0199, -0.0600,  0.0178,  0.0342,  0.0199,  0.0074,  0.0957,  0.0454,  0.0774,  0.0773]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0199, -0.0600,  0.0178,  0.0342,  0.0199,  0.0074,  0.0957,  0.0454,  0.0774,  0.0773],\n",
       "        [ 0.0173, -0.0610,  0.0199,  0.0422,  0.0199,  0.0120,  0.0979,  0.0445,  0.0834,  0.0714],\n",
       "        [ 0.0247, -0.0553,  0.0235,  0.0434,  0.0180,  0.0151,  0.1002,  0.0412,  0.0875,  0.0657],\n",
       "        [ 0.0212, -0.0555,  0.0222,  0.0429,  0.0194,  0.0131,  0.0981,  0.0431,  0.0867,  0.0695],\n",
       "        [ 0.0185, -0.0631,  0.0224,  0.0398,  0.0197,  0.0070,  0.0954,  0.0434,  0.0755,  0.0799],\n",
       "        [ 0.0225, -0.0621,  0.0233,  0.0359,  0.0187,  0.0090,  0.1021,  0.0469,  0.0811,  0.0740],\n",
       "        [ 0.0202, -0.0595,  0.0142,  0.0408,  0.0249,  0.0073,  0.1023,  0.0462,  0.0853,  0.0739],\n",
       "        [ 0.0199, -0.0582,  0.0255,  0.0397,  0.0194,  0.0133,  0.0969,  0.0474,  0.0849,  0.0710],\n",
       "        [ 0.0249, -0.0504,  0.0233,  0.0478,  0.0173,  0.0182,  0.0982,  0.0415,  0.0886,  0.0638],\n",
       "        [ 0.0179, -0.0626,  0.0210,  0.0426,  0.0194,  0.0156,  0.0923,  0.0419,  0.0886,  0.0695]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "1. Get batch from training set.\n",
    "2. Pass batch to network\n",
    "3. Calculate the loss (difference between the predicted values and true values.)\n",
    "4. Calculate the gradient of loss function w.r.t the network weights.\n",
    "5. Update the weights using the gradients to reduce the loss.\n",
    "6. Repeat steps 1-5 until one epoch is completed.\n",
    "7. Repeat until 1-6 until you reach a desired accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "batch = next(iter(train_loader))\n",
    "images, label = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3108673095703125"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(preds,labels) #calculate the loss\n",
    "loss.item() #A tensor will be returned so we call .item() to get the number from loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#only checking gradient on conv1 layer\n",
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-dbffb9089936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculating the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss.backward() #calculating the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN project on Fashion MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torchvison** for image processing\n",
    "**torchvision.transforms** for image transformation. It's a part of data agumentation\n",
    "given an image we rotate right, left, up, down. To bale to see all the possible permuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1715b8b0848>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120) # set number of outputlines\n",
    "torch.set_grad_enabled(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features =120)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features =60, out_features=10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # (1) Inpute layer\n",
    "        t = t\n",
    "        \n",
    "        #2 hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #3 hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #4 hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #5 hindden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #6 Output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load dataset to run the operations on it.<br>\n",
    "For this PyTorch provides two classes:<br>\n",
    "**torch.utils.Dataset**    -> An abstrach class for representing a dataset<br>\n",
    "**torch.utils.DataLoader** -> Wraps a dataset and provide access to underlying data.<br>\n",
    "An abstract class is a Python class that has methods we must implement, so we <br>\n",
    "can create custom dataset by creating a subclass that extends the functionality<br>\n",
    "of the Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '.data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with a single batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.303549289703369\n",
      "loss2: 2.272326707839966\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) #get batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) #pass batch\n",
    "loss = F.cross_entropy(preds,labels) #calclualte loss\n",
    "\n",
    "loss.backward() #calculate gradient\n",
    "optimizer.step() #update weights\n",
    "\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "print('loss2:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 45872 loss: 367.1552710235119\n",
      "epoch: 1 total_correct: 50879 loss: 246.03516413271427\n",
      "epoch: 2 total_correct: 51896 loss: 221.06047740578651\n",
      "epoch: 3 total_correct: 52122 loss: 211.60301105678082\n",
      "epoch: 4 total_correct: 52159 loss: 211.0542204529047\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader:   #Get batch\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "\n",
    "        '''each time when we compute the gradeint it will\n",
    "        add gradient to previous one. We don't need that \n",
    "        therefore we use optimizer.zero_grad() to zero out all gradients \n",
    "        from the layers'''\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  #Calculate gradients (back propagation)\n",
    "        '''The gradients are \"stored\" by the tensors themselves \n",
    "        (they have a grad and a requires_grad attributes)\n",
    "        once you call backward() on the loss.\n",
    "        After computing the gradients for all tensors in the model,\n",
    "        calling optimizer.step() makes the optimizer \n",
    "        iterate over all parameters (tensors) it is supposed to update\n",
    "        and use their internally stored grad to update their values.'''\n",
    "        \n",
    "        '''1. optimizer.step is performs a parameter update based on the current gradient\n",
    "        (stored in .grad attribute of a parameter) and the update rule.\n",
    "        2. Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter.\n",
    "        This is why you should call optimizer.zero_grad() after each .step() call.\n",
    "        Note that following the first .backward call,\n",
    "        a second call is only possible after you have performed another forward pass.\n",
    "        '''\n",
    "        optimizer.step() #update weights (Gradient descent)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "\n",
    "\n",
    "    print('epoch:',epoch, 'total_correct:', total_correct, 'loss:', total_loss)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing CNN results -- Builduing and Plotting a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting predictions for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model,loader):\n",
    "    #creating an empty tensor\n",
    "    all_preds = torch.tensor([])\n",
    "    '''we cannot give all the data, we need to give in batch so that\n",
    "    it is computer efficient'''\n",
    "    for batch in loader:\n",
    "        #here we are unpacking the batch in images and correspondin labels\n",
    "        \n",
    "        images,labels = batch\n",
    "        \n",
    "        preds = model(images)\n",
    "        #we are concatening the predictions to all_preds\n",
    "        all_preds = torch.cat((all_preds, preds),dim=0)\n",
    "    return all_preds\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bath size can vary the power of ur computer\n",
    "prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=500)\n",
    "'''give the network in prediciton method with prediction loader at the end\n",
    "it will give all the prediction sample in our training set\n",
    "the output is a tensor(train_preds)'''\n",
    "train_preds = get_all_preds(network, prediction_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    " print(train_preds.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CatBackward at 0x172db9e7dc8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''we have seen earlier that requires grad keeps the track of gradient\n",
    "but here we dont wanna do that without keeping the requires_grad = True'''\n",
    "#as simple as sound with torch no grads do these computations\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=500)\n",
    "    train_preds = get_all_preds(network,prediction_loader)\n",
    "    \n",
    "'''computationally its efficient and faster since it doesnt keep track of the graph'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(train_preds.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct 51607\n",
      "accuracy 0.8601166666666666\n"
     ]
    }
   ],
   "source": [
    "preds_correct = get_num_correct(train_preds,train_set.targets)\n",
    "print('total correct', preds_correct)\n",
    "print('accuracy', preds_correct/len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now we are ready to build confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 3,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack(\n",
    "    (\n",
    "    train_set.targets\n",
    "    ,train_preds.argmax(dim=1)\n",
    "    )\n",
    "    ,dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 9],\n",
       "        [0, 0],\n",
       "        [0, 3],\n",
       "        ...,\n",
       "        [3, 3],\n",
       "        [0, 0],\n",
       "        [5, 5]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 9]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are creating (10,10) confusion matrix because we have 10 categories\n",
    "cmt = torch.zeros(10,10, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in stacked:\n",
    "    tl,pl = p.tolist()\n",
    "    cmt[tl,pl] = cmt[tl,pl] + 1 #count the occurences of corresponding categories\n",
    "    #tl = true label\n",
    "    #pl = predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5279,    0,   94,  180,   16,    9,  388,    1,   33,    0],\n",
       "        [  54, 5733,   11,  122,    9,    2,   58,    0,   11,    0],\n",
       "        [  68,    2, 3545,   83, 1707,    5,  550,    2,   38,    0],\n",
       "        [ 226,   22,   16, 5494,   73,    8,  145,    0,   16,    0],\n",
       "        [   7,    5,  101,  432, 4980,    3,  433,    1,   38,    0],\n",
       "        [   1,    0,    0,    0,    0, 5843,    0,   99,   14,   43],\n",
       "        [1146,    2,  377,  202,  590,    0, 3612,    2,   69,    0],\n",
       "        [   1,    0,    0,    0,    0,  166,    1, 5723,   41,   68],\n",
       "        [  15,    1,   17,   27,   21,   28,   77,    6, 5808,    0],\n",
       "        [   0,    0,    1,    0,    0,   43,    0,  325,   41, 5590]], dtype=torch.int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from resources.plotcm import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5279,    0,   94,  180,   16,    9,  388,    1,   33,    0],\n",
       "       [  54, 5733,   11,  122,    9,    2,   58,    0,   11,    0],\n",
       "       [  68,    2, 3545,   83, 1707,    5,  550,    2,   38,    0],\n",
       "       [ 226,   22,   16, 5494,   73,    8,  145,    0,   16,    0],\n",
       "       [   7,    5,  101,  432, 4980,    3,  433,    1,   38,    0],\n",
       "       [   1,    0,    0,    0,    0, 5843,    0,   99,   14,   43],\n",
       "       [1146,    2,  377,  202,  590,    0, 3612,    2,   69,    0],\n",
       "       [   1,    0,    0,    0,    0,  166,    1, 5723,   41,   68],\n",
       "       [  15,    1,   17,   27,   21,   28,   77,    6, 5808,    0],\n",
       "       [   0,    0,    1,    0,    0,   43,    0,  325,   41, 5590]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack vs concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1,2,3])\n",
    "t2 = torch.tensor([2,3,4])\n",
    "t3 = torch.tensor([5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatening** joins sequence of tensors along an existing axis,<br>\n",
    "and **Stacking** joins sequence of tensors along new axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(\n",
    "    (t1,t2,t3)\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    (\n",
    "    t1.unsqueeze(0)\n",
    "    ,t2.unsqueeze(0)\n",
    "    ,t3.unsqueeze(0)\n",
    "    )\n",
    "    ,dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2],\n",
       "         [5]],\n",
       "\n",
       "        [[2],\n",
       "         [3],\n",
       "         [6]],\n",
       "\n",
       "        [[3],\n",
       "         [4],\n",
       "         [7]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(\n",
    "    (\n",
    "    t1.unsqueeze(1)\n",
    "    ,t2.unsqueeze(1)\n",
    "    ,t3.unsqueeze(1)\n",
    "    )\n",
    "    ,dim =1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
